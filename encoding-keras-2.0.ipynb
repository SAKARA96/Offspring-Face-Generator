{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "3.7.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,optimizers\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"datasets/ofg_family/\"\n",
    "randomiser = np.random.RandomState(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(family_dir):\n",
    "    dic={}\n",
    "    sub=[a for a in listdir(path+\"/\"+family_dir)]\n",
    "    \n",
    "    for ele in sub:\n",
    "        mypath = path+\"/\"+family_dir+\"/\"+ele+\"/\"\n",
    "        onlyfiles = [mypath+f for f in listdir(mypath)]\n",
    "        \n",
    "        addr = randomiser.choice(onlyfiles)\n",
    "        original_img = np.array(Image.open(addr).resize((64,64),Image.ANTIALIAS))\n",
    "        if ele[0].lower()=='f':\n",
    "            dic['father'] = original_img\n",
    "        elif ele[0].lower()=='m':\n",
    "            dic['mother'] = original_img\n",
    "        elif ele.lower()=='child_male':\n",
    "            dic['child'] = original_img    \n",
    "            dic['gender']=np.zeros((original_img.shape))\n",
    "        elif ele.lower()=='child_female':\n",
    "            dic['child'] = original_img    \n",
    "            dic['gender'] = np.ones((original_img.shape))\n",
    "    return [dic['father'],dic['mother'],dic['gender'],dic['child']]\n",
    "\n",
    "def generate_batch(families_batch):\n",
    "    np_images=[]\n",
    "    \n",
    "    for family in families_batch:\n",
    "        res = generate_image(family)\n",
    "        if( res != None):\n",
    "            np_images.append(res)\n",
    "    \n",
    "    return np_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full_data = load_npdata(path)\n",
    "#print(len(full_data))\n",
    "\n",
    "for r, d, f in os.walk(path):\n",
    "        all_families=d\n",
    "        break\n",
    "\n",
    "randomiser.shuffle(all_families)\n",
    "\n",
    "train_families = all_families[:-500]\n",
    "test_families = all_families[-500:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create encoder decoder supervised pipeling operation\n",
    "input: stacked images of father and mother (32,32,6)\n",
    "encoder -> decoder -> image reconstruction\n",
    "reconstruction loss w/ target image as child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = tf.keras.layers.Concatenate()\n",
    "\n",
    "family_data = generate_image(all_families[0])\n",
    "inp = concat([family_data[0],family_data[1]])\n",
    "child = family_data[3]\n",
    "child = tf.convert_to_tensor(child, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02) \n",
    "  \n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer,\n",
    "                             use_bias=True))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                   use_bias=True))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_nostride(filters, size):\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=1,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                   use_bias=False))\n",
    "\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "\n",
    "    result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd_random_normal_init = 0.135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncoderNN():\n",
    "  down_stack = [\n",
    "    downsample(32, 8, apply_batchnorm=True), #32x32x32\n",
    "    downsample(64, 8, apply_batchnorm=False),#16x16x64\n",
    "    downsample(128, 8, apply_batchnorm=False)#8x8x128\n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    upsample(128, 8),#16x16x128\n",
    "    upsample(64, 8),#32x32x64\n",
    "    upsample(32, 8),  #64x64x32\n",
    "#     upsample_nostride(16,4),\n",
    "#     upsample_nostride(8,4)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., sd_random_normal_init)\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')\n",
    "\n",
    "  concat = tf.keras.layers.Concatenate()\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[64,64,6])\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = concat([x, skip])\n",
    "    \n",
    "  x = last(x)\n",
    "\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = tf.keras.layers.Concatenate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_optimizer = tf.keras.optimizers.Adam(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_array(tensor1):\n",
    "    return tensor1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_encoder(inp_batch, target_batch,b_size):\n",
    "    with tf.GradientTape() as enc_tape:\n",
    "        \n",
    "        inp_batch = tf.unstack(inp_batch)\n",
    "        target_batch = tf.unstack(target_batch)\n",
    "\n",
    "        encoder_loss =tf.Variable(0,dtype='float32')\n",
    "\n",
    "        for idx,input_image in enumerate(inp_batch):\n",
    "            gen_output = encoder(input_image[tf.newaxis,...], training=True)\n",
    "            encoder_loss = encoder_loss + tf.reduce_mean(tf.abs(target_batch[idx] - gen_output))\n",
    "            \n",
    "\n",
    "        encoder_loss/=b_size\n",
    "    \n",
    "    print(\"ENCODER_LOSS: \",tensor_to_array(encoder_loss))\n",
    "    #calculate gradients\n",
    "    encoder_gradients = enc_tape.gradient(encoder_loss,encoder.trainable_variables)\n",
    "\n",
    "    #apply gradients on optimizer\n",
    "    encoder_optimizer.apply_gradients(zip(encoder_gradients,encoder.trainable_variables))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_encoder(train_ds, epochs, test_ds, batch):\n",
    "    losses=np.array([])\n",
    "    for epoch in range(epochs):\n",
    "        print(\"______________________________EPOCH %d_______________________________\"%(epoch+1))\n",
    "        start = time.time()\n",
    "        for i in range(len(train_ds)//batch):\n",
    "            loss_i =np.array([])\n",
    "            batch_data = np.asarray(generate_batch(train_ds[i*batch:(i+1)*batch]))\n",
    "            batch_data = batch_data / 255 * 2 -1\n",
    "            \n",
    "            print(\"Generated batch\", batch_data.shape)\n",
    "\n",
    "            X_train = tf.convert_to_tensor(concat([batch_data[:,0],batch_data[:,1]]),dtype =tf.float32)\n",
    "            Y_train = tf.convert_to_tensor(batch_data[:,3],dtype =tf.float32)\n",
    "            \n",
    "            train_encoder(X_train,Y_train,batch)\n",
    "            \n",
    "            print(\"Trained for batch %d/%d\"%(i+1,(len(train_ds)//batch)))\n",
    "    print(\"______________________________TRAINING COMPLETED_______________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________EPOCH 1_______________________________\n",
      "Generated batch (250, 4, 64, 64, 3)\n",
      "ENCODER_LOSS:  0.75173724\n",
      "Trained for batch 1/6\n",
      "Generated batch (250, 4, 64, 64, 3)\n",
      "ENCODER_LOSS:  0.7152041\n",
      "Trained for batch 2/6\n",
      "Generated batch (250, 4, 64, 64, 3)\n",
      "ENCODER_LOSS:  0.66041934\n",
      "Trained for batch 3/6\n",
      "Generated batch (250, 4, 64, 64, 3)\n",
      "ENCODER_LOSS:  0.6185677\n",
      "Trained for batch 4/6\n",
      "Generated batch (250, 4, 64, 64, 3)\n",
      "ENCODER_LOSS:  0.57408905\n",
      "Trained for batch 5/6\n",
      "Generated batch (250, 4, 64, 64, 3)\n",
      "ENCODER_LOSS:  0.5370635\n",
      "Trained for batch 6/6\n",
      "______________________________TRAINING COMPLETED_______________________________\n"
     ]
    }
   ],
   "source": [
    "train_dataset = all_families[:-500]\n",
    "test_dataset = all_families[-500:]\n",
    "EPOCHS = 1\n",
    "batch = 250\n",
    "encoder = EncoderNN()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    fit_encoder(train_dataset, EPOCHS, test_dataset,batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "family_data = generate_image(all_families[700])\n",
    "inp = concat([family_data[0],family_data[1]])\n",
    "inp = tf.cast(inp, tf.float32)\n",
    "gen_output = encoder(inp[tf.newaxis,...], training=False)\n",
    "temp = gen_output.numpy()\n",
    "plt.imshow(np.squeeze(temp))\n",
    "# print(temp)\n",
    "print(np.amin(temp))\n",
    "print(np.amax(temp))\n",
    "# print(encoder.trainable_weights[0][0][0][0])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# epochs = 1\n",
    "batch = 500\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # print(\"Epoch \", epoch , \" .....\")\n",
    "    for i in range(len(train_families)//batch):\n",
    "        batch_data = np.asarray(generate_batch(train_families[i*batch:(i+1)*batch]))\n",
    "        batch_data = batch_data / 255 * 2 -1\n",
    "        # print(\"Generated batch\", batch_data.shape)\n",
    "        \n",
    "        X_train = tf.convert_to_tensor(concat([batch_data[:,0],batch_data[:,1]]),dtype =tf.float32)\n",
    "        # print(\"Batch converted to tensor\")\n",
    "        \n",
    "        Y_train = batch_data[:,3]\n",
    "        history = encoder.fit(X_train, Y_train, batch_size=batch)\n",
    "print(\"Training DONE!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "family_data = generate_image(all_families[1])\n",
    "inp = concat([family_data[0],family_data[1]])\n",
    "inp = tf.cast(inp, tf.float32)\n",
    "gen_output = encoder(inp[tf.newaxis,...], training=False)\n",
    "temp = gen_output.numpy()\n",
    "temp = np.squeeze(temp)\n",
    "plt.imshow(temp)\n",
    "\n",
    "print(np.amin(temp))\n",
    "print(np.amax(temp))\n",
    "print(encoder.trainable_weights[0][0][0][0])\n",
    "0.00196027\n",
    "# plt.imshow((gen_output[0]+1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., sd_random_normal_init)\n",
    "\n",
    "    inp = tf.keras.layers.Input(shape=[64, 64, 6], name='input_image')\n",
    "    tar = tf.keras.layers.Input(shape=[64, 64, 3], name='target_image')\n",
    "\n",
    "    x = tf.reshape(tf.keras.layers.concatenate([inp, tar]),(1,64,64,9))\n",
    "    down1 = downsample(32, 4, True)(x)\n",
    "    down2 = downsample(64, 4,True)(down1) \n",
    "    down3 = downsample(128, 4,True)(down2) \n",
    "\n",
    "#     zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) \n",
    "    conv = tf.keras.layers.Conv2D(256, 3, strides=1,\n",
    "                                kernel_initializer=initializer,\n",
    "                                use_bias=True)(down3)\n",
    "\n",
    "    batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
    "\n",
    "    leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
    "\n",
    "#     zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu)\n",
    "\n",
    "    last = tf.keras.layers.Conv2D(1, 3, strides=1,\n",
    "                                kernel_initializer=initializer)(leaky_relu) \n",
    "    return tf.keras.Model(inputs=[inp, tar], outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_output = encoder(inp[tf.newaxis,...], training=False)\n",
    "\n",
    "target = family_data[2]\n",
    "discriminator = Discriminator()\n",
    "disc_out = discriminator([inp, tf.squeeze(gen_output)], training=False)\n",
    "print(disc_out.numpy().shape)\n",
    "plt.imshow(disc_out[0,...,-1], vmin=-20, vmax=20, cmap='RdBu_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_array(tensor1):\n",
    "    return tensor1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output,dtype=tf.float32), disc_real_output)\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output,dtype=tf.float32), disc_generated_output)\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = loss_object(tf.ones_like(disc_generated_output,dtype=tf.float32), disc_generated_output)\n",
    "  # mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
    "    return total_gen_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(inp_batch, target_batch,b_size):\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        \n",
    "        inp_batch = tf.unstack(inp_batch)\n",
    "        target_batch = tf.unstack(target_batch)\n",
    "\n",
    "        gen_loss =tf.Variable(0,dtype='float32')\n",
    "        disc_loss =tf.Variable(0,dtype='float32')\n",
    "\n",
    "        for idx,input_image in enumerate(inp_batch):\n",
    "            gen_output = encoder(input_image[tf.newaxis,...], training=True)\n",
    "            disc_real_output = discriminator([input_image, target_batch[idx]], training=True)\n",
    "            disc_generated_output = discriminator([input_image, tf.squeeze(gen_output)], training=True)\n",
    "            gen_loss = gen_loss + generator_loss(disc_generated_output, gen_output, target_batch[idx])\n",
    "            disc_loss = disc_loss + discriminator_loss(disc_real_output, disc_generated_output)\n",
    "\n",
    "\n",
    "        gen_loss/=b_size\n",
    "        disc_loss/=b_size\n",
    "        \n",
    "    print(\"GEN_LOSS\",tensor_to_array(gen_loss))\n",
    "    print(\"DISC_LOSS\",tensor_to_array(disc_loss))\n",
    "\n",
    "    generator_gradients = gen_tape.gradient(gen_loss,encoder.trainable_variables)\n",
    "    discriminator_gradients = disc_tape.gradient(disc_loss,discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
    "                                          encoder.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
    "                                              discriminator.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(train_ds, epochs, test_ds,batch):\n",
    "    for epoch in range(epochs):\n",
    "        print(\"______________________________EPOCH %d_______________________________\"%(epoch))\n",
    "        start = time.time()\n",
    "        for i in range(len(train_ds)//batch):\n",
    "            batch_data = np.asarray(generate_batch(train_ds[i*batch:(i+1)*batch]))\n",
    "\n",
    "            print(\"Generated batch\", batch_data.shape)\n",
    "\n",
    "            X_train = tf.convert_to_tensor(concat([batch_data[:,0],batch_data[:,1]]),dtype =tf.float32)\n",
    "#             print(\"Xtrain\",X_train.shape)\n",
    "#             print(\"Batch converted to tensor\")\n",
    "\n",
    "            Y_train = tf.convert_to_tensor(batch_data[:,3],dtype =tf.float32)\n",
    "            train_step(X_train,Y_train,batch)\n",
    "            print(\"Trained for batch %d/%d\"%(i+1,(len(train_ds)//batch)))\n",
    "    print(\"______________________________TRAINING COMPLETED_______________________________\")\n",
    "    checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = all_families[:-500]\n",
    "test_dataset = all_families[-500:]\n",
    "EPOCHS = 1\n",
    "batch = 300\n",
    "encoder = EncoderNN()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoint'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=encoder,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "    fit(train_dataset, EPOCHS, test_dataset,batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "family_data = generate_image(all_families[502])\n",
    "inp = concat([family_data[0],family_data[1]])\n",
    "inp = tf.cast(inp, tf.float32)\n",
    "gen_output = encoder(inp[tf.newaxis,...], training=False)\n",
    "print(tf.reduce_min(gen_output))\n",
    "print(tf.reduce_max(gen_output))\n",
    "plt.imshow(gen_output[0,...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-3.7",
   "language": "python",
   "name": "gpu-3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
