{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,optimizers\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"C:/DL_Dataset/Project_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_npdata(path):\n",
    "    for r, d, f in os.walk(path):\n",
    "        direc=d\n",
    "        break\n",
    "    rs = np.random.RandomState(123)\n",
    "    rs.shuffle(direc)\n",
    "    direc = direc[:len(direc)-500]\n",
    "    \n",
    "    np_images=[]\n",
    "    start= time.time()\n",
    "    for d in direc:\n",
    "        dic={}\n",
    "        sub=[a for a in listdir(path+\"/\"+d)]\n",
    "        flag=0\n",
    "        for ele in sub:\n",
    "            mypath = path+\"/\"+d+\"/\"+ele+\"/\"\n",
    "            onlyfiles = [mypath+f for f in listdir(mypath)]\n",
    "            temp=[]\n",
    "            for addr in onlyfiles:\n",
    "#                 temp.append(np.array(Image.open(addr)))\n",
    "                original_img = np.array(Image.open(addr))\n",
    "                downscaled_img = tf.compat.v1.image.resize_image_with_pad(original_img,32,32)\n",
    "                temp.append(downscaled_img)\n",
    "            if len(temp)>0:\n",
    "                if ele[0].lower()=='f':\n",
    "                    dic['father']=temp\n",
    "                elif ele[0].lower()=='m':\n",
    "                    dic['mother']=temp\n",
    "                elif ele.lower()=='child_male':\n",
    "                    dic['child']=temp    \n",
    "                    dic['gender']=np.zeros((temp[0].shape))\n",
    "                elif ele.lower()=='child_female':\n",
    "                    dic['child']=temp    \n",
    "                    dic['gender']=np.ones((temp[0].shape))    \n",
    "            else:\n",
    "                flag=1\n",
    "                break\n",
    "\n",
    "        if flag!=1:\n",
    "            for x in dic['father']:\n",
    "                for y in dic['mother']:\n",
    "                    for z in dic['child']:\n",
    "                        np_images.append([x,y,dic['gender'],z])\n",
    "    print(\"time:\",time.time()-start)\n",
    "    return np_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 117.30267071723938\n",
      "1089290\n"
     ]
    }
   ],
   "source": [
    "full_data = load_npdata(path)\n",
    "print(len(full_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create encoder decoder supervised pipeling operation\n",
    "input: stacked images of father and mother (32,32,6)\n",
    "encoder -> decoder -> image reconstruction\n",
    "reconstruction loss w/ target image as child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = tf.keras.layers.Concatenate()\n",
    "inp = concat([full_data[0][0],full_data[0][1]])\n",
    "child = full_data[0][3]\n",
    "inp = tf.convert_to_tensor(inp, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample(filters, size, apply_batchnorm=True):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "\n",
    "  if apply_batchnorm:\n",
    "    result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  result.add(tf.keras.layers.LeakyReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample(filters, size, apply_dropout=False):\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "  result = tf.keras.Sequential()\n",
    "  result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                    use_bias=False))\n",
    "\n",
    "  result.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "  if apply_dropout:\n",
    "      result.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  result.add(tf.keras.layers.ReLU())\n",
    "\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EncoderNN():\n",
    "  down_stack = [\n",
    "    downsample(3, 4, apply_batchnorm=True),\n",
    "    downsample(3, 4, apply_batchnorm=False),\n",
    "    downsample(3, 4, apply_batchnorm=False) \n",
    "  ]\n",
    "\n",
    "  up_stack = [\n",
    "    upsample(3, 4, apply_dropout=True),\n",
    "    upsample(3, 4, apply_dropout=False),\n",
    "    upsample(3, 4, apply_dropout=False)\n",
    "  ]\n",
    "\n",
    "  initializer = tf.random_normal_initializer(0., 0.02)\n",
    "  last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh') # (bs, 256, 256, 3)\n",
    "\n",
    "  concat = tf.keras.layers.Concatenate()\n",
    "\n",
    "  inputs = tf.keras.layers.Input(shape=[32,32,6])\n",
    "  x = inputs\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = []\n",
    "  for down in down_stack:\n",
    "    x = down(x)\n",
    "    skips.append(x)\n",
    "\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    x = concat([x, skip])\n",
    "    \n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = EncoderNN()\n",
    "\n",
    "# gen_output = encoder(inp[tf.newaxis,...], training=False)\n",
    "# loss= tf.reduce_mean(tf.image.total_variation(tf.concat([child,tf.squeeze(gen_output)],axis=0)))\n",
    "# loss = tf.reduce_mean(tf.nn.l2_loss(child-gen_output))\n",
    "# print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss():\n",
    "    def loss(y_true,y_pred):\n",
    "         print(y_true.shape)\n",
    "         print(y_pred.shape)\n",
    "         return tf.reduce_mean(tf.image.total_variation(tf.concat([y_true,y_pred],axis=0)))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inp = concat([full_data[0][0],full_data[0][1]])\n",
    "inp = tf.convert_to_tensor(inp, dtype=tf.float32)\n",
    "X_train = tf.reshape(inp,([1,32,32,6]))\n",
    "Y_train = tf.reshape(full_data[0][3],(1,32,32,3))\n",
    "\n",
    "encoder.compile(loss=custom_loss(),\n",
    "              optimizer=tf.keras.optimizers.RMSprop())\n",
    "history = encoder.fit(X_train, Y_train,\n",
    "                    batch_size=1,\n",
    "                    epochs=1)\n",
    "\n",
    "gen_output = encoder(inp[tf.newaxis,...], training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "concat = tf.keras.layers.Concatenate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None, None, None)\n",
      "(None, 32, 32, 3)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-174-60734d4331e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mX_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\gpu-3.7\\lib\\site-packages\\numpy\\core\\numeric.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    537\u001b[0m     \"\"\"\n\u001b[1;32m--> 538\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "encoder.compile(loss=custom_loss(),optimizer=tf.keras.optimizers.RMSprop())\n",
    "\n",
    "divide = 50000\n",
    "batch = 1000\n",
    "epoch = 5\n",
    "for i in range(len(full_data)//divide):\n",
    "    test= np.asarray(full_data[i*divide:(i+1)*divide])\n",
    "    print(\"Data divided\")\n",
    "    X_train = tf.convert_to_tensor(concat([test[:,0],test[:,1]]),dtype =tf.float32)\n",
    "    print(\"Batch converted to tensor\")\n",
    "    Y_train = test[:,3]\n",
    "    history = encoder.fit(X_train, Y_train,batch_size=batch,epochs=epoch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-3.7",
   "language": "python",
   "name": "gpu-3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
