{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "3.7.5\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from PIL import Image\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers,models,optimizers\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_to_array(tensor1):\n",
    "    return tensor1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"datasets/child_PreGen_data/\"\n",
    "randomiser = np.random.RandomState(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, d, f in os.walk(path):\n",
    "    all_child = f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(child_batch):\n",
    "    np_images=[]\n",
    "    for child in child_batch:\n",
    "        res = np.array(Image.open(path+child))\n",
    "        np_images.append(res)\n",
    "    return np_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 64\n",
    "mean = 0.\n",
    "std_dev = 0.02\n",
    "lr = 0.00001\n",
    "b1 = 0.9\n",
    "b2 = 0.99\n",
    "sd_random_normal_init = 0.02\n",
    "\n",
    "EPOCHS = 300\n",
    "batch = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_gen_upsample(filters, size,apply_batchnorm = False):\n",
    "    initializer = tf.random_normal_initializer(mean, std_dev)\n",
    "\n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
    "                                    padding='same',\n",
    "                                    kernel_initializer=initializer,\n",
    "                                   use_bias=False))\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        result.add(tf.keras.layers.ELU())\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_Generator():\n",
    "    \n",
    "    up_stack_noise = [\n",
    "    pre_gen_upsample(64,4,apply_batchnorm=True), #8x8x64\n",
    "    pre_gen_upsample(64,4,apply_batchnorm=True), #16x16x64\n",
    "    pre_gen_upsample(32,4,apply_batchnorm=True)  #32x32x32\n",
    "    ]\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(mean, sd_random_normal_init)\n",
    "    last = tf.keras.layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n",
    "                                         strides=2,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='tanh')\n",
    "    \n",
    "    noise = tf.keras.layers.Input(shape=(4,4,64))\n",
    "    \n",
    "    x = noise\n",
    "    for up in up_stack_noise:\n",
    "        x = up(x)\n",
    "#     print(x.shape)\n",
    "    output = last(x)\n",
    "#     print(output.shape)\n",
    "    return tf.keras.Model(inputs=noise, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_disc_downsample(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(mean, std_dev) \n",
    "  \n",
    "    result = tf.keras.Sequential()\n",
    "    result.add(\n",
    "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer,\n",
    "                             use_bias=False))\n",
    "\n",
    "    if apply_batchnorm:\n",
    "        result.add(tf.keras.layers.BatchNormalization())\n",
    "        \n",
    "    result.add(tf.keras.layers.LeakyReLU(alpha = 0.2))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pre_Discriminator():\n",
    "    \n",
    "    down_stack_image = [\n",
    "    pre_disc_downsample(32,4,apply_batchnorm=False), #32x32x32\n",
    "    pre_disc_downsample(64,4,apply_batchnorm=True), #16x16x64\n",
    "    pre_disc_downsample(64,4,apply_batchnorm=True),  #8x8x64\n",
    "    pre_disc_downsample(256,4,apply_batchnorm=False) #4x4x256\n",
    "        \n",
    "    ]\n",
    "    \n",
    "    initializer = tf.random_normal_initializer(mean, sd_random_normal_init)\n",
    "    last = tf.keras.layers.Conv2DTranspose(1, 4,\n",
    "                                         strides=1,\n",
    "                                         padding='same',\n",
    "                                         kernel_initializer=initializer,\n",
    "                                         activation='sigmoid')\n",
    "    \n",
    "    image = tf.keras.layers.Input(shape=(64,64,3))\n",
    "    print(image.shape)\n",
    "    x = image\n",
    "    for down in down_stack_image:\n",
    "        x = down(x)\n",
    "        print(x.shape)\n",
    "    output = last(x)\n",
    "#     print(output.shape)\n",
    "\n",
    "    return tf.keras.Model(inputs=image, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_gen_optimizer = tf.keras.optimizers.Adam(learning_rate = lr, beta_1=b1, beta_2= b2)\n",
    "pre_disc_optimizer = tf.keras.optimizers.Adam(learning_rate = lr, beta_1=b1, beta_2= b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_PreGAN(noise_batch, target_batch, b_size):\n",
    "    with tf.GradientTape() as pre_gen_tape ,tf.GradientTape() as pre_disc_tape:\n",
    "        \n",
    "        pre_gen_outputs = pre_gen(noise_batch, training=True)\n",
    "        disc_pre_gen_out = pre_disc(pre_gen_outputs, training=True)\n",
    "        disc_target_out = pre_disc(target_batch,training=True)\n",
    "        ones = tf.ones_like(disc_pre_gen_out)\n",
    "        zeros = tf.zeros_like(disc_pre_gen_out)\n",
    "        \n",
    "        disc_loss = bce(ones,disc_target_out) + bce(zeros,disc_pre_gen_out)\n",
    "        gen_loss = bce(ones,disc_pre_gen_out)\n",
    "        \n",
    "    \n",
    "    print(\"GEN_LOSS: \",tensor_to_array(gen_loss))\n",
    "    print(\"DISC_LOSS: \",tensor_to_array(disc_loss))\n",
    "    \n",
    "    #calculate gradients\n",
    "    pre_gen_gradients = pre_gen_tape.gradient(gen_loss,pre_gen.trainable_variables)\n",
    "    pre_disc_gradients = pre_disc_tape.gradient(disc_loss,pre_disc.trainable_variables)\n",
    "    \n",
    "    #apply gradients on optimizer\n",
    "    pre_gen_optimizer.apply_gradients(zip(pre_gen_gradients,pre_gen.trainable_variables))\n",
    "    pre_disc_optimizer.apply_gradients(zip(pre_disc_gradients,pre_disc.trainable_variables))\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_loss_history = []\n",
    "disc_loss_history = []\n",
    "def fit_encoder(all_child, epochs, batch):\n",
    "    no_of_iter = len(all_child)//batch\n",
    "    for epoch in range(epochs):\n",
    "        print(\"______________________________EPOCH %d_______________________________\"%(epoch+1))\n",
    "        start = time.time()\n",
    "        gen_epoch_loss = 0.0\n",
    "        disc_epoch_loss = 0.0\n",
    "        \n",
    "        for i in range(no_of_iter):\n",
    "            batch_data = np.asarray(generate_batch(all_child[i*batch:(i+1)*batch]))\n",
    "            child_batch= batch_data / 255 * 2 -1\n",
    "            \n",
    "            \n",
    "            print(\"Generated batch\", batch_data.shape)\n",
    "            \n",
    "            noise = tf.random.normal((batch,4,4,64),mean= 0.0, stddev=1.0)\n",
    "\n",
    "            gen_batch_loss, disc_batch_loss = train_PreGAN(noise, child_batch,batch)\n",
    "            gen_epoch_loss += gen_batch_loss\n",
    "            disc_epoch_loss += disc_batch_loss\n",
    "            print(\"Trained for batch %d/%d\"%(i+1,(len(all_child)//batch)))\n",
    "            \n",
    "        gen_epoch_loss /= no_of_iter\n",
    "        disc_epoch_loss /= no_of_iter\n",
    "        \n",
    "        if (epoch%1 == 0):\n",
    "            gen_loss_history.append(gen_epoch_loss)\n",
    "            disc_loss_history.append(disc_epoch_loss)\n",
    "        \n",
    "        pre_gen.save_weights(filepath='checkpoint/pre_gan_weights/epoch_%d_preG.pth' % (epoch))\n",
    "        pre_disc.save_weights(filepath='checkpoint/pre_gan_weights/epoch_%d_preD.pth' % (epoch))\n",
    "        \n",
    "        \n",
    "    print(\"______________________________TRAINING COMPLETED_______________________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 3)\n",
      "(None, 32, 32, 32)\n",
      "(None, 16, 16, 64)\n",
      "(None, 8, 8, 64)\n",
      "(None, 4, 4, 256)\n",
      "______________________________EPOCH 1_______________________________\n",
      "Generated batch (64, 64, 64, 3)\n",
      "WARNING:tensorflow:Layer sequential_17 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "GEN_LOSS:  0.7224448\n",
      "DISC_LOSS:  1.4064783\n",
      "Trained for batch 1/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.72377616\n",
      "DISC_LOSS:  1.4009968\n",
      "Trained for batch 2/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.71947706\n",
      "DISC_LOSS:  1.3820184\n",
      "Trained for batch 3/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7249042\n",
      "DISC_LOSS:  1.3921268\n",
      "Trained for batch 4/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7264246\n",
      "DISC_LOSS:  1.3639123\n",
      "Trained for batch 5/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7117466\n",
      "DISC_LOSS:  1.3805423\n",
      "Trained for batch 6/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.71227324\n",
      "DISC_LOSS:  1.3651199\n",
      "Trained for batch 7/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7157607\n",
      "DISC_LOSS:  1.3623941\n",
      "Trained for batch 8/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.71231604\n",
      "DISC_LOSS:  1.347739\n",
      "Trained for batch 9/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7130567\n",
      "DISC_LOSS:  1.3381653\n",
      "Trained for batch 10/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7202648\n",
      "DISC_LOSS:  1.3295932\n",
      "Trained for batch 11/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7101469\n",
      "DISC_LOSS:  1.3217025\n",
      "Trained for batch 12/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.71050835\n",
      "DISC_LOSS:  1.3235946\n",
      "Trained for batch 13/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.71750146\n",
      "DISC_LOSS:  1.3183306\n",
      "Trained for batch 14/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7190898\n",
      "DISC_LOSS:  1.3018358\n",
      "Trained for batch 15/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.721368\n",
      "DISC_LOSS:  1.2962685\n",
      "Trained for batch 16/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.71760404\n",
      "DISC_LOSS:  1.2984781\n",
      "Trained for batch 17/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7235712\n",
      "DISC_LOSS:  1.2857771\n",
      "Trained for batch 18/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7207247\n",
      "DISC_LOSS:  1.2720025\n",
      "Trained for batch 19/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7302128\n",
      "DISC_LOSS:  1.2665696\n",
      "Trained for batch 20/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7283574\n",
      "DISC_LOSS:  1.2684709\n",
      "Trained for batch 21/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.74091\n",
      "DISC_LOSS:  1.2540319\n",
      "Trained for batch 22/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.73740095\n",
      "DISC_LOSS:  1.2669448\n",
      "Trained for batch 23/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.73415524\n",
      "DISC_LOSS:  1.2563493\n",
      "Trained for batch 24/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.74956214\n",
      "DISC_LOSS:  1.2375529\n",
      "Trained for batch 25/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7498195\n",
      "DISC_LOSS:  1.2191348\n",
      "Trained for batch 26/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7506104\n",
      "DISC_LOSS:  1.2235606\n",
      "Trained for batch 27/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7549057\n",
      "DISC_LOSS:  1.217584\n",
      "Trained for batch 28/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7613841\n",
      "DISC_LOSS:  1.2104094\n",
      "Trained for batch 29/29\n",
      "______________________________EPOCH 2_______________________________\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7703035\n",
      "DISC_LOSS:  1.2144465\n",
      "Trained for batch 1/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7631697\n",
      "DISC_LOSS:  1.2159419\n",
      "Trained for batch 2/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.76311404\n",
      "DISC_LOSS:  1.1900547\n",
      "Trained for batch 3/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7748351\n",
      "DISC_LOSS:  1.2060069\n",
      "Trained for batch 4/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7863667\n",
      "DISC_LOSS:  1.1706622\n",
      "Trained for batch 5/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7785421\n",
      "DISC_LOSS:  1.181562\n",
      "Trained for batch 6/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7877469\n",
      "DISC_LOSS:  1.1642172\n",
      "Trained for batch 7/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.79604167\n",
      "DISC_LOSS:  1.1777935\n",
      "Trained for batch 8/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.7970538\n",
      "DISC_LOSS:  1.159354\n",
      "Trained for batch 9/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8096376\n",
      "DISC_LOSS:  1.1496707\n",
      "Trained for batch 10/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8058424\n",
      "DISC_LOSS:  1.1565998\n",
      "Trained for batch 11/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.81026626\n",
      "DISC_LOSS:  1.1329317\n",
      "Trained for batch 12/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.81175435\n",
      "DISC_LOSS:  1.140404\n",
      "Trained for batch 13/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.82438314\n",
      "DISC_LOSS:  1.1361823\n",
      "Trained for batch 14/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.824069\n",
      "DISC_LOSS:  1.1222882\n",
      "Trained for batch 15/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.82770014\n",
      "DISC_LOSS:  1.1184547\n",
      "Trained for batch 16/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.82290614\n",
      "DISC_LOSS:  1.1304897\n",
      "Trained for batch 17/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8345795\n",
      "DISC_LOSS:  1.109452\n",
      "Trained for batch 18/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8398462\n",
      "DISC_LOSS:  1.0825964\n",
      "Trained for batch 19/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.84865427\n",
      "DISC_LOSS:  1.0816495\n",
      "Trained for batch 20/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.84545195\n",
      "DISC_LOSS:  1.1001949\n",
      "Trained for batch 21/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.85933566\n",
      "DISC_LOSS:  1.0803906\n",
      "Trained for batch 22/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8532157\n",
      "DISC_LOSS:  1.1103244\n",
      "Trained for batch 23/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.85717523\n",
      "DISC_LOSS:  1.0883527\n",
      "Trained for batch 24/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8732084\n",
      "DISC_LOSS:  1.0660565\n",
      "Trained for batch 25/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.87234116\n",
      "DISC_LOSS:  1.0429976\n",
      "Trained for batch 26/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.87206787\n",
      "DISC_LOSS:  1.0608506\n",
      "Trained for batch 27/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.86930627\n",
      "DISC_LOSS:  1.0546813\n",
      "Trained for batch 28/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8749505\n",
      "DISC_LOSS:  1.0499773\n",
      "Trained for batch 29/29\n",
      "______________________________EPOCH 3_______________________________\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8841078\n",
      "DISC_LOSS:  1.0629101\n",
      "Trained for batch 1/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8815638\n",
      "DISC_LOSS:  1.0559236\n",
      "Trained for batch 2/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8865497\n",
      "DISC_LOSS:  1.0196235\n",
      "Trained for batch 3/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8866297\n",
      "DISC_LOSS:  1.0516964\n",
      "Trained for batch 4/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.8958748\n",
      "DISC_LOSS:  1.0160482\n",
      "Trained for batch 5/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.9021211\n",
      "DISC_LOSS:  1.011682\n",
      "Trained for batch 6/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.90419006\n",
      "DISC_LOSS:  1.0007875\n",
      "Trained for batch 7/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.89821315\n",
      "DISC_LOSS:  1.0361998\n",
      "Trained for batch 8/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.91393995\n",
      "DISC_LOSS:  1.0011512\n",
      "Trained for batch 9/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.9139056\n",
      "DISC_LOSS:  1.0075808\n",
      "Trained for batch 10/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.9076797\n",
      "DISC_LOSS:  1.0189207\n",
      "Trained for batch 11/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.91470253\n",
      "DISC_LOSS:  0.98255396\n",
      "Trained for batch 12/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.9056489\n",
      "DISC_LOSS:  1.0053995\n",
      "Trained for batch 13/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.92005336\n",
      "DISC_LOSS:  0.99823695\n",
      "Trained for batch 14/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.93123364\n",
      "DISC_LOSS:  0.9766865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained for batch 15/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.95179284\n",
      "DISC_LOSS:  0.96293145\n",
      "Trained for batch 16/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.94573104\n",
      "DISC_LOSS:  0.98416376\n",
      "Trained for batch 17/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.94478\n",
      "DISC_LOSS:  0.9685334\n",
      "Trained for batch 18/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.94414186\n",
      "DISC_LOSS:  0.93866324\n",
      "Trained for batch 19/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.9400586\n",
      "DISC_LOSS:  0.94838524\n",
      "Trained for batch 20/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.96080405\n",
      "DISC_LOSS:  0.9629292\n",
      "Trained for batch 21/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.966348\n",
      "DISC_LOSS:  0.942146\n",
      "Trained for batch 22/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.96516776\n",
      "DISC_LOSS:  0.97813785\n",
      "Trained for batch 23/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.96167684\n",
      "DISC_LOSS:  0.95671964\n",
      "Trained for batch 24/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.9715103\n",
      "DISC_LOSS:  0.93961394\n",
      "Trained for batch 25/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.98147726\n",
      "DISC_LOSS:  0.9040553\n",
      "Trained for batch 26/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0102799\n",
      "DISC_LOSS:  0.91621804\n",
      "Trained for batch 27/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.98958397\n",
      "DISC_LOSS:  0.9211389\n",
      "Trained for batch 28/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  0.9917939\n",
      "DISC_LOSS:  0.91657394\n",
      "Trained for batch 29/29\n",
      "______________________________EPOCH 4_______________________________\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0019748\n",
      "DISC_LOSS:  0.93355626\n",
      "Trained for batch 1/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0079176\n",
      "DISC_LOSS:  0.91975564\n",
      "Trained for batch 2/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0108972\n",
      "DISC_LOSS:  0.8809486\n",
      "Trained for batch 3/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.008739\n",
      "DISC_LOSS:  0.9233727\n",
      "Trained for batch 4/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0138625\n",
      "DISC_LOSS:  0.8885522\n",
      "Trained for batch 5/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0087225\n",
      "DISC_LOSS:  0.88613826\n",
      "Trained for batch 6/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0274987\n",
      "DISC_LOSS:  0.86380756\n",
      "Trained for batch 7/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0343695\n",
      "DISC_LOSS:  0.90446985\n",
      "Trained for batch 8/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0227894\n",
      "DISC_LOSS:  0.87601376\n",
      "Trained for batch 9/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0325855\n",
      "DISC_LOSS:  0.8867852\n",
      "Trained for batch 10/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0400258\n",
      "DISC_LOSS:  0.8882731\n",
      "Trained for batch 11/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.050389\n",
      "DISC_LOSS:  0.84704006\n",
      "Trained for batch 12/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0449395\n",
      "DISC_LOSS:  0.8707328\n",
      "Trained for batch 13/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0375388\n",
      "DISC_LOSS:  0.8733649\n",
      "Trained for batch 14/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0556679\n",
      "DISC_LOSS:  0.848233\n",
      "Trained for batch 15/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0438936\n",
      "DISC_LOSS:  0.8555871\n",
      "Trained for batch 16/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0814257\n",
      "DISC_LOSS:  0.86245614\n",
      "Trained for batch 17/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0590715\n",
      "DISC_LOSS:  0.8506383\n",
      "Trained for batch 18/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0687422\n",
      "DISC_LOSS:  0.81372666\n",
      "Trained for batch 19/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0771884\n",
      "DISC_LOSS:  0.81462204\n",
      "Trained for batch 20/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0835826\n",
      "DISC_LOSS:  0.8494705\n",
      "Trained for batch 21/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0829371\n",
      "DISC_LOSS:  0.82167095\n",
      "Trained for batch 22/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0848413\n",
      "DISC_LOSS:  0.8645679\n",
      "Trained for batch 23/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1012168\n",
      "DISC_LOSS:  0.83081746\n",
      "Trained for batch 24/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1101722\n",
      "DISC_LOSS:  0.81775177\n",
      "Trained for batch 25/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1177838\n",
      "DISC_LOSS:  0.77918303\n",
      "Trained for batch 26/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1085459\n",
      "DISC_LOSS:  0.8188642\n",
      "Trained for batch 27/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.113128\n",
      "DISC_LOSS:  0.8082805\n",
      "Trained for batch 28/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1036494\n",
      "DISC_LOSS:  0.8125154\n",
      "Trained for batch 29/29\n",
      "______________________________EPOCH 5_______________________________\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.135777\n",
      "DISC_LOSS:  0.8246205\n",
      "Trained for batch 1/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.0976005\n",
      "DISC_LOSS:  0.82197165\n",
      "Trained for batch 2/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.124218\n",
      "DISC_LOSS:  0.7700181\n",
      "Trained for batch 3/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1060808\n",
      "DISC_LOSS:  0.82794\n",
      "Trained for batch 4/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1398358\n",
      "DISC_LOSS:  0.78000474\n",
      "Trained for batch 5/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1492538\n",
      "DISC_LOSS:  0.7681432\n",
      "Trained for batch 6/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1391425\n",
      "DISC_LOSS:  0.75536156\n",
      "Trained for batch 7/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.172157\n",
      "DISC_LOSS:  0.80351174\n",
      "Trained for batch 8/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1756953\n",
      "DISC_LOSS:  0.75393057\n",
      "Trained for batch 9/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.150198\n",
      "DISC_LOSS:  0.78882575\n",
      "Trained for batch 10/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.149473\n",
      "DISC_LOSS:  0.7955203\n",
      "Trained for batch 11/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1793205\n",
      "DISC_LOSS:  0.7405734\n",
      "Trained for batch 12/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1809199\n",
      "DISC_LOSS:  0.7687394\n",
      "Trained for batch 13/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1731479\n",
      "DISC_LOSS:  0.763615\n",
      "Trained for batch 14/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1733842\n",
      "DISC_LOSS:  0.7477918\n",
      "Trained for batch 15/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1771216\n",
      "DISC_LOSS:  0.74532545\n",
      "Trained for batch 16/29\n",
      "Generated batch (64, 64, 64, 3)\n",
      "GEN_LOSS:  1.1794546\n",
      "DISC_LOSS:  0.7795945\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-5976e3f362a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpre_disc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPre_Discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/cpu:0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mfit_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_child\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mgen_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdisc_loss_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-f0ae07c4fb52>\u001b[0m in \u001b[0;36mfit_encoder\u001b[0;34m(all_child, epochs, batch)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mgen_batch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc_batch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_PreGAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchild_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mgen_epoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgen_batch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mdisc_epoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdisc_batch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-a5b523a31ccc>\u001b[0m in \u001b[0;36mtrain_PreGAN\u001b[0;34m(noise_batch, target_batch, b_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m#calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpre_gen_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_gen_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpre_gen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mpre_disc_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_disc_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpre_disc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DBackpropInputGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m     63\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m           data_format=op.get_attr(\"data_format\").decode())\n\u001b[0m\u001b[1;32m     66\u001b[0m   ]\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1022\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1025\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pre_gen = Pre_Generator()\n",
    "pre_disc = Pre_Discriminator()\n",
    "with tf.device('/cpu:0'):\n",
    "    fit_encoder(all_child, EPOCHS,batch)\n",
    "    gen_loss_history = tf.stack(gen_loss_history)\n",
    "    disc_loss_history = tf.stack(disc_loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.72690606 0.82254714 0.93350214 1.0563481 ]\n",
      "[1.3071617 1.1273994 0.9822624 0.8583172]\n"
     ]
    }
   ],
   "source": [
    "gen_loss_history = gen_loss_history.numpy()\n",
    "disc_loss_history = disc_loss_history.numpy()\n",
    "#add plot for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = tf.random.normal((1,4,4,64),mean= 0.0, stddev=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_gen = Pre_Generator()\n",
    "pre_disc = Pre_Discriminator()\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    gen_output = pre_gen(noise, training=True)\n",
    "temp = gen_output.numpy()\n",
    "plt.imshow(np.squeeze(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_gen.load_weights('checkpoint/pre_gan_weights/epoch_20_preG.pth')\n",
    "pre_disc.load_weights('checkpoint/pre_gan_weights/epoch_20_preD.pth')\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    gen_output = pre_gen(noise, training=True)\n",
    "temp = gen_output.numpy()\n",
    "plt.imshow(np.squeeze(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_gen.load_weights('checkpoint/pre_gan_weights/epoch_40_preG.pth')\n",
    "pre_disc.load_weights('checkpoint/pre_gan_weights/epoch_40_preD.pth')\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    gen_output = pre_gen(noise, training=True)\n",
    "temp = gen_output.numpy()\n",
    "plt.imshow(np.squeeze(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
